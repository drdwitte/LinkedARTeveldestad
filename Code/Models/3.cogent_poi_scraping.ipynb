{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcd61d0a-c840-487d-a717-19f646d31242",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google_images_download\n",
      "  Downloading google_images_download-2.8.0.tar.gz (14 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting selenium\n",
      "  Downloading selenium-4.8.2-py3-none-any.whl (6.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m0m\n",
      "\u001b[?25hRequirement already satisfied: urllib3[socks]~=1.26 in /opt/conda/lib/python3.10/site-packages (from selenium->google_images_download) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in /opt/conda/lib/python3.10/site-packages (from selenium->google_images_download) (2022.12.7)\n",
      "Collecting trio-websocket~=0.9\n",
      "  Downloading trio_websocket-0.10.2-py3-none-any.whl (17 kB)\n",
      "Collecting trio~=0.17\n",
      "  Downloading trio-0.22.0-py3-none-any.whl (384 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m384.9/384.9 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sortedcontainers in /opt/conda/lib/python3.10/site-packages (from trio~=0.17->selenium->google_images_download) (2.4.0)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.10/site-packages (from trio~=0.17->selenium->google_images_download) (3.4)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /opt/conda/lib/python3.10/site-packages (from trio~=0.17->selenium->google_images_download) (22.2.0)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from trio~=0.17->selenium->google_images_download) (1.3.0)\n",
      "Collecting outcome\n",
      "  Downloading outcome-1.2.0-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting exceptiongroup>=1.0.0rc9\n",
      "  Downloading exceptiongroup-1.1.1-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: async-generator>=1.9 in /opt/conda/lib/python3.10/site-packages (from trio~=0.17->selenium->google_images_download) (1.10)\n",
      "Collecting wsproto>=0.14\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from urllib3[socks]~=1.26->selenium->google_images_download) (1.7.1)\n",
      "Collecting h11<1,>=0.9.0\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: google_images_download\n",
      "  Building wheel for google_images_download (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for google_images_download: filename=google_images_download-2.8.0-py2.py3-none-any.whl size=14534 sha256=c205819e21fd4e5ec4f988161ceb4dc01a267bd0f4d148f0477ce2f15e98544b\n",
      "  Stored in directory: /home/kymillev/.cache/pip/wheels/5b/f2/64/0eecb22fee5cbc0321d332492ddcc45b03c6373b5616ee95c8\n",
      "Successfully built google_images_download\n",
      "Installing collected packages: outcome, h11, exceptiongroup, wsproto, trio, trio-websocket, selenium, google_images_download\n",
      "Successfully installed exceptiongroup-1.1.1 google_images_download-2.8.0 h11-0.14.0 outcome-1.2.0 selenium-4.8.2 trio-0.22.0 trio-websocket-0.10.2 wsproto-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install google_images_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a37e5ac-c032-4fbd-9953-efaef5e3d3a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import re \n",
    "import json\n",
    "import requests\n",
    "import shutil \n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import h5py\n",
    "\n",
    "from lavis.models import load_model_and_preprocess\n",
    "from annoy import AnnoyIndex\n",
    "#from google_images_download import google_images_download  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314e3d8d-9be9-4827-a2c7-7bdda4183042",
   "metadata": {},
   "source": [
    "### Scraping code is rather buggy, altered from here:\n",
    "https://github.com/hardikvasa/google-images-download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e4a33c8-a3c6-419c-8f69-0f6bc713aa68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# In[ ]:\n",
    "#  coding: utf-8\n",
    "\n",
    "###### Searching and Downloading Google Images to the local disk ######\n",
    "\n",
    "# Import Libraries\n",
    "import sys\n",
    "import selenium.common.exceptions\n",
    "\n",
    "version = (3, 0)\n",
    "cur_version = sys.version_info\n",
    "if cur_version >= version:  # If the Current Version of Python is 3.0 or above\n",
    "    import urllib.request\n",
    "    from urllib.request import Request, urlopen\n",
    "    from urllib.request import URLError, HTTPError\n",
    "    from urllib.parse import quote\n",
    "    import http.client\n",
    "    from http.client import IncompleteRead, BadStatusLine\n",
    "\n",
    "    http.client._MAXHEADERS = 1000\n",
    "else:  # If the Current Version of Python is 2.x\n",
    "    import urllib2\n",
    "    from urllib2 import Request, urlopen\n",
    "    from urllib2 import URLError, HTTPError\n",
    "    from urllib import quote\n",
    "    import httplib\n",
    "    from httplib import IncompleteRead, BadStatusLine\n",
    "\n",
    "    httplib._MAXHEADERS = 1000\n",
    "import time  # Importing the time library to check the time of code execution\n",
    "import os\n",
    "import argparse\n",
    "import ssl\n",
    "import datetime\n",
    "import json\n",
    "import re\n",
    "import codecs\n",
    "import socket\n",
    "\n",
    "args_list = [\"keywords\", \"keywords_from_file\", \"prefix_keywords\", \"suffix_keywords\",\n",
    "             \"limit\", \"format\", \"color\", \"color_type\", \"usage_rights\", \"size\",\n",
    "             \"exact_size\", \"aspect_ratio\", \"type\", \"time\", \"time_range\", \"delay\", \"url\", \"single_image\",\n",
    "             \"output_directory\", \"image_directory\", \"no_directory\", \"proxy\", \"similar_images\", \"specific_site\",\n",
    "             \"print_urls\", \"print_size\", \"print_paths\", \"metadata\", \"extract_metadata\", \"socket_timeout\",\n",
    "             \"thumbnail\", \"thumbnail_only\", \"language\", \"prefix\", \"chromedriver\", \"browser\", \"related_images\", \"safe_search\",\n",
    "             \"no_numbering\",\n",
    "             \"offset\", \"no_download\", \"save_source\", \"silent_mode\", \"ignore_urls\"]\n",
    "\n",
    "\n",
    "def user_input():\n",
    "    config = argparse.ArgumentParser()\n",
    "    config.add_argument('-cf', '--config_file', help='config file name', default='', type=str, required=False)\n",
    "    config_file_check = config.parse_known_args()\n",
    "    object_check = vars(config_file_check[0])\n",
    "\n",
    "    if object_check['config_file'] != '':\n",
    "        records = []\n",
    "        json_file = json.load(open(config_file_check[0].config_file))\n",
    "        for record in range(0, len(json_file['Records'])):\n",
    "            arguments = {}\n",
    "            for i in args_list:\n",
    "                arguments[i] = None\n",
    "            for key, value in json_file['Records'][record].items():\n",
    "                arguments[key] = value\n",
    "            records.append(arguments)\n",
    "        records_count = len(records)\n",
    "    else:\n",
    "        # Taking command line arguments from users\n",
    "        parser = argparse.ArgumentParser()\n",
    "        parser.add_argument('-k', '--keywords', help='delimited list input', type=str, required=False)\n",
    "        parser.add_argument('-kf', '--keywords_from_file', help='extract list of keywords from a text file', type=str,\n",
    "                            required=False)\n",
    "        parser.add_argument('-sk', '--suffix_keywords',\n",
    "                            help='comma separated additional words added after to main keyword', type=str,\n",
    "                            required=False)\n",
    "        parser.add_argument('-pk', '--prefix_keywords',\n",
    "                            help='comma separated additional words added before main keyword', type=str, required=False)\n",
    "        parser.add_argument('-l', '--limit', help='delimited list input', type=str, required=False)\n",
    "        parser.add_argument('-f', '--format', help='download images with specific format', type=str, required=False,\n",
    "                            choices=['jpg', 'gif', 'png', 'bmp', 'svg', 'webp', 'ico'])\n",
    "        parser.add_argument('-u', '--url', help='search with google image URL', type=str, required=False)\n",
    "        parser.add_argument('-x', '--single_image', help='downloading a single image from URL', type=str,\n",
    "                            required=False)\n",
    "        parser.add_argument('-o', '--output_directory', help='download images in a specific main directory', type=str,\n",
    "                            required=False)\n",
    "        parser.add_argument('-i', '--image_directory', help='download images in a specific sub-directory', type=str,\n",
    "                            required=False)\n",
    "        parser.add_argument('-n', '--no_directory', default=False,\n",
    "                            help='download images in the main directory but no sub-directory', action=\"store_true\")\n",
    "        parser.add_argument('-d', '--delay', help='delay in seconds to wait between downloading two images', type=int,\n",
    "                            required=False)\n",
    "        parser.add_argument('-co', '--color', help='filter on color', type=str, required=False,\n",
    "                            choices=['red', 'orange', 'yellow', 'green', 'teal', 'blue', 'purple', 'pink', 'white',\n",
    "                                     'gray', 'black', 'brown'])\n",
    "        parser.add_argument('-ct', '--color_type', help='filter on color', type=str, required=False,\n",
    "                            choices=['full-color', 'black-and-white', 'transparent'])\n",
    "        parser.add_argument('-r', '--usage_rights', help='usage rights', type=str, required=False,\n",
    "                            choices=['labeled-for-reuse-with-modifications', 'labeled-for-reuse',\n",
    "                                     'labeled-for-noncommercial-reuse-with-modification',\n",
    "                                     'labeled-for-nocommercial-reuse'])\n",
    "        parser.add_argument('-s', '--size', help='image size', type=str, required=False,\n",
    "                            choices=['large', 'medium', 'icon', '>400*300', '>640*480', '>800*600', '>1024*768', '>2MP',\n",
    "                                     '>4MP', '>6MP', '>8MP', '>10MP', '>12MP', '>15MP', '>20MP', '>40MP', '>70MP'])\n",
    "        parser.add_argument('-es', '--exact_size', help='exact image resolution \"WIDTH,HEIGHT\"', type=str,\n",
    "                            required=False)\n",
    "        parser.add_argument('-t', '--type', help='image type', type=str, required=False,\n",
    "                            choices=['face', 'photo', 'clipart', 'line-drawing', 'animated'])\n",
    "        parser.add_argument('-w', '--time', help='image age', type=str, required=False,\n",
    "                            choices=['past-24-hours', 'past-7-days', 'past-month', 'past-year'])\n",
    "        parser.add_argument('-wr', '--time_range',\n",
    "                            help='time range for the age of the image. should be in the format {\"time_min\":\"YYYY-MM-DD\",\"time_max\":\"YYYY-MM-DD\"}',\n",
    "                            type=str, required=False)\n",
    "        parser.add_argument('-a', '--aspect_ratio', help='comma separated additional words added to keywords', type=str,\n",
    "                            required=False,\n",
    "                            choices=['tall', 'square', 'wide', 'panoramic'])\n",
    "        parser.add_argument('-si', '--similar_images',\n",
    "                            help='downloads images very similar to the image URL you provide', type=str, required=False)\n",
    "        parser.add_argument('-ss', '--specific_site', help='downloads images that are indexed from a specific website',\n",
    "                            type=str, required=False)\n",
    "        parser.add_argument('-p', '--print_urls', default=False, help=\"Print the URLs of the images\",\n",
    "                            action=\"store_true\")\n",
    "        parser.add_argument('-ps', '--print_size', default=False, help=\"Print the size of the images on disk\",\n",
    "                            action=\"store_true\")\n",
    "        parser.add_argument('-pp', '--print_paths', default=False,\n",
    "                            help=\"Prints the list of absolute paths of the images\", action=\"store_true\")\n",
    "        parser.add_argument('-m', '--metadata', default=False, help=\"Print the metadata of the image\",\n",
    "                            action=\"store_true\")\n",
    "        parser.add_argument('-e', '--extract_metadata', default=False, help=\"Dumps all the logs into a text file\",\n",
    "                            action=\"store_true\")\n",
    "        parser.add_argument('-st', '--socket_timeout', default=False,\n",
    "                            help=\"Connection timeout waiting for the image to download\", type=float)\n",
    "        parser.add_argument('-th', '--thumbnail', default=False,\n",
    "                            help=\"Downloads image thumbnail along with the actual image\", action=\"store_true\")\n",
    "        parser.add_argument('-tho', '--thumbnail_only', default=False,\n",
    "                            help=\"Downloads only thumbnail without downloading actual images\", action=\"store_true\")\n",
    "        parser.add_argument('-la', '--language', default=False,\n",
    "                            help=\"Defines the language filter. The search results are authomatically returned in that language\",\n",
    "                            type=str, required=False,\n",
    "                            choices=['Arabic', 'Chinese (Simplified)', 'Chinese (Traditional)', 'Czech', 'Danish',\n",
    "                                     'Dutch', 'English', 'Estonian', 'Finnish', 'French', 'German', 'Greek', 'Hebrew',\n",
    "                                     'Hungarian', 'Icelandic', 'Italian', 'Japanese', 'Korean', 'Latvian', 'Lithuanian',\n",
    "                                     'Norwegian', 'Portuguese', 'Polish', 'Romanian', 'Russian', 'Spanish', 'Swedish',\n",
    "                                     'Turkish'])\n",
    "        parser.add_argument('-pr', '--prefix', default=False,\n",
    "                            help=\"A word that you would want to prefix in front of each image name\", type=str,\n",
    "                            required=False)\n",
    "        parser.add_argument('-px', '--proxy', help='specify a proxy address and port', type=str, required=False)\n",
    "        parser.add_argument('-cd', '--chromedriver',\n",
    "                            help='specify the path to chromedriver executable in your local machine', type=str,\n",
    "                            required=False)\n",
    "        parser.add_argument('-wb', '--browser',\n",
    "                            help='Specify which driver to use', type=str,\n",
    "                            required=False)\n",
    "        parser.add_argument('-ri', '--related_images', default=False,\n",
    "                            help=\"Downloads images that are similar to the keyword provided\", action=\"store_true\")\n",
    "        parser.add_argument('-sa', '--safe_search', default=False,\n",
    "                            help=\"Turns on the safe search filter while searching for images\", action=\"store_true\")\n",
    "        parser.add_argument('-nn', '--no_numbering', default=False,\n",
    "                            help=\"Allows you to exclude the default numbering of images\", action=\"store_true\")\n",
    "        parser.add_argument('-of', '--offset', help=\"Where to start in the fetched links\", type=str, required=False)\n",
    "        parser.add_argument('-nd', '--no_download', default=False,\n",
    "                            help=\"Prints the URLs of the images and/or thumbnails without downloading them\",\n",
    "                            action=\"store_true\")\n",
    "        parser.add_argument('-iu', '--ignore_urls', default=False,\n",
    "                            help=\"delimited list input of image urls/keywords to ignore\", type=str)\n",
    "        parser.add_argument('-sil', '--silent_mode', default=False,\n",
    "                            help=\"Remains silent. Does not print notification messages on the terminal\",\n",
    "                            action=\"store_true\")\n",
    "        parser.add_argument('-is', '--save_source',\n",
    "                            help=\"creates a text file containing a list of downloaded images along with source page url\",\n",
    "                            type=str, required=False)\n",
    "\n",
    "        args = parser.parse_args()\n",
    "        arguments = vars(args)\n",
    "        records = []\n",
    "        records.append(arguments)\n",
    "    return records\n",
    "\n",
    "\n",
    "class googleimagesdownload:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def _extract_data_pack(self, page):\n",
    "        start_line = page.find(\"AF_initDataCallback({key: \\\\'ds:1\\\\'\") - 10\n",
    "        start_object = page.find('[', start_line + 1)\n",
    "        end_object = page.rfind(']',0,page.find('</script>', start_object + 1))+1\n",
    "        object_raw = str(page[start_object:end_object])\n",
    "        return bytes(object_raw, \"utf-8\").decode(\"unicode_escape\")\n",
    "\n",
    "    def _extract_data_pack_extended(self, page):\n",
    "        start_line = page.find(\"AF_initDataCallback({key: 'ds:1'\") - 10\n",
    "        start_object = page.find('[', start_line + 1)\n",
    "        end_object = page.rfind(']',0,page.find('</script>', start_object + 1)) + 1\n",
    "        return str(page[start_object:end_object])\n",
    "\n",
    "    def _extract_data_pack_ajax(self, data):\n",
    "        lines = data.split('\\n')\n",
    "        return json.loads(lines[3])[0][2]\n",
    "\n",
    "    @staticmethod\n",
    "    def _image_objects_from_pack(data):\n",
    "        image_data = json.loads(data)\n",
    "        # NOTE: google sometimes changes their format, breaking this. set a breakpoint here to find the correct index\n",
    "        grid = image_data[56][-1][0][-1][-1][0]\n",
    "        image_objects = []\n",
    "        for item in grid:\n",
    "            obj = list(item[0][0].values())[0]\n",
    "            # ads and carousels will be empty\n",
    "            if not obj or not obj[1]:\n",
    "                continue\n",
    "            image_objects.append(obj)\n",
    "        return image_objects\n",
    "\n",
    "    # Downloading entire Web Document (Raw Page Content)\n",
    "    def download_page(self, url):\n",
    "        version = (3, 0)\n",
    "        cur_version = sys.version_info\n",
    "        headers = {}\n",
    "        headers[\n",
    "            'User-Agent'] = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.104 Safari/537.36\"\n",
    "        if cur_version >= version:  # If the Current Version of Python is 3.0 or above\n",
    "            try:\n",
    "                req = urllib.request.Request(url, headers=headers)\n",
    "                resp = urllib.request.urlopen(req)\n",
    "                respData = str(resp.read())\n",
    "            except:\n",
    "                print(\"Could not open URL. Please check your internet connection and/or ssl settings \\n\"\n",
    "                      \"If you are using proxy, make sure your proxy settings is configured correctly\")\n",
    "                sys.exit()\n",
    "        else:  # If the Current Version of Python is 2.x\n",
    "            try:\n",
    "                req = urllib2.Request(url, headers=headers)\n",
    "                try:\n",
    "                    response = urllib2.urlopen(req)\n",
    "                except URLError:  # Handling SSL certificate failed\n",
    "                    context = ssl._create_unverified_context()\n",
    "                    response = urlopen(req, context=context)\n",
    "                respData = response.read()\n",
    "            except:\n",
    "                print(\"Could not open URL. Please check your internet connection and/or ssl settings \\n\"\n",
    "                      \"If you are using proxy, make sure your proxy settings is configured correctly\")\n",
    "                sys.exit()\n",
    "                return \"Page Not found\"\n",
    "        try:\n",
    "            return self._image_objects_from_pack(self._extract_data_pack(respData)), self.get_all_tabs(respData)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print('Image objects data unpacking failed. Please leave a comment with the above error at https://github.com/Joeclinton1/google-images-download/pull/26')\n",
    "            sys.exit()\n",
    "\n",
    "    # Download Page for more than 100 images\n",
    "    def download_extended_page(self, url, chromedriver, browser):\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.common.keys import Keys\n",
    "        if sys.version_info[0] < 3:\n",
    "            reload(sys)\n",
    "            sys.setdefaultencoding('utf8')\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument('--no-sandbox')\n",
    "        options.add_argument(\"--headless\")\n",
    "\n",
    "        if browser == 'Firefox':\n",
    "            browser = webdriver.Firefox()\n",
    "        else:\n",
    "            try:\n",
    "                browser = webdriver.Chrome(chromedriver, chrome_options=options)\n",
    "            except Exception as e:\n",
    "                print(\"Looks like we cannot locate the path the 'chromedriver' (use the '--chromedriver' \"\n",
    "                      \"argument to specify the path to the executable.) or google chrome browser is not \"\n",
    "                      \"installed on your machine (exception: %s)\" % e)\n",
    "                sys.exit()\n",
    "        browser.set_window_size(1024, 768)\n",
    "\n",
    "        # Open the link\n",
    "        browser.get(url)\n",
    "        browser.execute_script(\"\"\"\n",
    "            (function(XHR){\n",
    "                \"use strict\";\n",
    "                var open = XHR.prototype.open;\n",
    "                var send = XHR.prototype.send;\n",
    "                var data = [];\n",
    "                XHR.prototype.open = function(method, url, async, user, pass) {\n",
    "                    this._url = url;\n",
    "                    open.call(this, method, url, async, user, pass);\n",
    "                }\n",
    "                XHR.prototype.send = function(data) {\n",
    "                    var self = this;\n",
    "                    var url = this._url;\n",
    "                    function stateChanged() {\n",
    "                        if (self.readyState == 4) {\n",
    "                            console.log(\"data available for: \" + url)\n",
    "                            XHR.prototype._data.push(self.response);\n",
    "                        }\n",
    "                    }\n",
    "                    if (url.includes(\"/batchexecute?\")) {\n",
    "                        this.addEventListener(\"readystatechange\", stateChanged, false);\n",
    "                    }\n",
    "                    send.call(this, data);\n",
    "                };\n",
    "                XHR.prototype._data = [];\n",
    "            })(XMLHttpRequest);\n",
    "        \"\"\")\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "        # Bypass \"Before you continue\" if it appears\n",
    "        try:\n",
    "            browser.find_element_by_css_selector(\"[aria-label='Accept all']\").click()\n",
    "            time.sleep(1)\n",
    "        except selenium.common.exceptions.NoSuchElementException:\n",
    "            pass\n",
    "\n",
    "        print(\"Getting you a lot of images. This may take a few moments...\")\n",
    "\n",
    "        element = browser.find_element_by_tag_name(\"body\")\n",
    "        # Scroll down\n",
    "        for i in range(50):\n",
    "            element.send_keys(Keys.PAGE_DOWN)\n",
    "            time.sleep(0.3)\n",
    "\n",
    "        try:\n",
    "            browser.find_element_by_xpath('//input[@value=\"Show more results\"]').click()\n",
    "            for i in range(50):\n",
    "                element.send_keys(Keys.PAGE_DOWN)\n",
    "                time.sleep(0.3)  # bot id protection\n",
    "        except:\n",
    "            for i in range(10):\n",
    "                element.send_keys(Keys.PAGE_DOWN)\n",
    "                time.sleep(0.3)  # bot id protection\n",
    "\n",
    "        print(\"Reached end of Page.\")\n",
    "        time.sleep(0.5)\n",
    "\n",
    "        source = browser.page_source  # page source\n",
    "        images = self._image_objects_from_pack(self._extract_data_pack_extended(source))\n",
    "\n",
    "        ajax_data = browser.execute_script(\"return XMLHttpRequest.prototype._data\") # I think this is broken\n",
    "        for chunk in ajax_data if ajax_data else []:\n",
    "            images += self._image_objects_from_pack(self._extract_data_pack_ajax(chunk))\n",
    "\n",
    "        # close the browser\n",
    "        browser.close()\n",
    "\n",
    "        return images, self.get_all_tabs(source)\n",
    "\n",
    "    # Correcting the escape characters for python2\n",
    "    def replace_with_byte(self, match):\n",
    "        return chr(int(match.group(0)[1:], 8))\n",
    "\n",
    "    def repair(self, brokenjson):\n",
    "        invalid_escape = re.compile(r'\\\\[0-7]{1,3}')  # up to 3 digits for byte values up to FF\n",
    "        return invalid_escape.sub(self.replace_with_byte, brokenjson)\n",
    "\n",
    "    # Finding 'Next Image' from the given raw page\n",
    "    def get_next_tab(self, s):\n",
    "        start_line = s.find('class=\"dtviD\"')\n",
    "        if start_line == -1:  # If no links are found then give an error!\n",
    "            end_quote = 0\n",
    "            link = \"no_tabs\"\n",
    "            return link, '', end_quote\n",
    "        else:\n",
    "            start_line = s.find('class=\"dtviD\"')\n",
    "            start_content = s.find('href=\"', start_line + 1)\n",
    "            end_content = s.find('\">', start_content + 1)\n",
    "            url_item = \"https://www.google.com\" + str(s[start_content + 6:end_content])\n",
    "            url_item = url_item.replace('&amp;', '&')\n",
    "\n",
    "            start_line_2 = s.find('class=\"dtviD\"')\n",
    "            s = s.replace('&amp;', '&')\n",
    "            start_content_2 = s.find(':', start_line_2 + 1)\n",
    "            end_content_2 = s.find('&usg=', start_content_2 + 1)\n",
    "            url_item_name = str(s[start_content_2 + 1:end_content_2])\n",
    "\n",
    "            chars = url_item_name.find(',g_1:')\n",
    "            chars_end = url_item_name.find(\":\", chars + 6)\n",
    "            if chars_end == -1:\n",
    "                updated_item_name = (url_item_name[chars + 5:]).replace(\"+\", \" \")\n",
    "            else:\n",
    "                updated_item_name = (url_item_name[chars + 5:chars_end]).replace(\"+\", \" \")\n",
    "\n",
    "            return url_item, updated_item_name, end_content\n",
    "\n",
    "    # Getting all links with the help of '_images_get_next_image'\n",
    "    def get_all_tabs(self, page):\n",
    "        tabs = {}\n",
    "        while True:\n",
    "            item, item_name, end_content = self.get_next_tab(page)\n",
    "            if item == \"no_tabs\":\n",
    "                break\n",
    "            else:\n",
    "                if len(item_name) > 100 or item_name == \"background-color\":\n",
    "                    break\n",
    "                else:\n",
    "                    tabs[item_name] = item  # Append all the links in the list named 'Links'\n",
    "                    time.sleep(0.1)  # Timer could be used to slow down the request for image downloads\n",
    "                    page = page[end_content:]\n",
    "        return tabs\n",
    "\n",
    "    # Format the object in readable format\n",
    "    def format_object(self, object):\n",
    "        data = object[1]\n",
    "        main = data[3]\n",
    "        info = data[9]\n",
    "        if info is None:\n",
    "            info = data[11]\n",
    "        if info is None:\n",
    "            info = data[23]\n",
    "        formatted_object = {}\n",
    "        try:\n",
    "            formatted_object['image_height'] = main[2]\n",
    "            formatted_object['image_width'] = main[1]\n",
    "            formatted_object['image_link'] = main[0]\n",
    "            formatted_object['image_format'] = main[0][-1 * (len(main[0]) - main[0].rfind(\".\") - 1):]\n",
    "            formatted_object['image_description'] = info['2003'][3]\n",
    "            formatted_object['image_host'] = info['2003'][17]\n",
    "            formatted_object['image_source'] = info['2003'][2]\n",
    "            formatted_object['image_thumbnail_url'] = data[2][0]\n",
    "        except Exception as e:\n",
    "            #print(e)\n",
    "            try:\n",
    "                \n",
    "                formatted_object['image_link'] = main[0]\n",
    "                formatted_object['image_format'] = main[0][-1 * (len(main[0]) - main[0].rfind(\".\") - 1):]\n",
    "                formatted_object['image_source'] = ''\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                return None\n",
    "            \n",
    "        return formatted_object\n",
    "\n",
    "    # function to download single image\n",
    "    def single_image(self, image_url):\n",
    "        main_directory = \"downloads\"\n",
    "        extensions = (\".jpg\", \".gif\", \".png\", \".bmp\", \".svg\", \".webp\", \".ico\")\n",
    "        url = image_url\n",
    "        try:\n",
    "            os.makedirs(main_directory)\n",
    "        except OSError as e:\n",
    "            if e.errno != 17:\n",
    "                raise\n",
    "            pass\n",
    "        req = Request(url, headers={\n",
    "            \"User-Agent\": \"Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.17 (KHTML, like Gecko) Chrome/24.0.1312.27 Safari/537.17\"})\n",
    "\n",
    "        response = urlopen(req, None, 10)\n",
    "        data = response.read()\n",
    "        response.close()\n",
    "\n",
    "        image_name = str(url[(url.rfind('/')) + 1:])\n",
    "        if '?' in image_name:\n",
    "            image_name = image_name[:image_name.find('?')]\n",
    "        # if \".jpg\" in image_name or \".gif\" in image_name or \".png\" in image_name or \".bmp\" in image_name or \".svg\" in image_name or \".webp\" in image_name or \".ico\" in image_name:\n",
    "        if any(map(lambda extension: extension in image_name, extensions)):\n",
    "            file_name = main_directory + \"/\" + image_name\n",
    "        else:\n",
    "            file_name = main_directory + \"/\" + image_name + \".jpg\"\n",
    "            image_name = image_name + \".jpg\"\n",
    "\n",
    "        try:\n",
    "            output_file = open(file_name, 'wb')\n",
    "            output_file.write(data)\n",
    "            output_file.close()\n",
    "        except IOError as e:\n",
    "            raise e\n",
    "        except OSError as e:\n",
    "            raise e\n",
    "        print(\"completed ====> \" + image_name.encode('raw_unicode_escape').decode('utf-8'))\n",
    "        return\n",
    "\n",
    "    def similar_images(self, similar_images):\n",
    "        version = (3, 0)\n",
    "        cur_version = sys.version_info\n",
    "        if cur_version >= version:  # If the Current Version of Python is 3.0 or above\n",
    "            try:\n",
    "                searchUrl = 'https://www.google.com/searchbyimage?site=search&sa=X&image_url=' + similar_images\n",
    "                headers = {}\n",
    "                headers[\n",
    "                    'User-Agent'] = \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36\"\n",
    "\n",
    "                req1 = urllib.request.Request(searchUrl, headers=headers)\n",
    "                resp1 = urllib.request.urlopen(req1)\n",
    "                content = str(resp1.read())\n",
    "                l1 = content.find('AMhZZ')\n",
    "                l2 = content.find('&', l1)\n",
    "                urll = content[l1:l2]\n",
    "\n",
    "                newurl = \"https://www.google.com/search?tbs=sbi:\" + urll + \"&site=search&sa=X\"\n",
    "                req2 = urllib.request.Request(newurl, headers=headers)\n",
    "                resp2 = urllib.request.urlopen(req2)\n",
    "                l3 = content.find('/search?sa=X&amp;q=')\n",
    "                l4 = content.find(';', l3 + 19)\n",
    "                urll2 = content[l3 + 19:l4]\n",
    "                return urll2\n",
    "            except:\n",
    "                return \"Cloud not connect to Google Images endpoint\"\n",
    "        else:  # If the Current Version of Python is 2.x\n",
    "            try:\n",
    "                searchUrl = 'https://www.google.com/searchbyimage?site=search&sa=X&image_url=' + similar_images\n",
    "                headers = {}\n",
    "                headers[\n",
    "                    'User-Agent'] = \"Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.17 (KHTML, like Gecko) Chrome/24.0.1312.27 Safari/537.17\"\n",
    "\n",
    "                req1 = urllib2.Request(searchUrl, headers=headers)\n",
    "                resp1 = urllib2.urlopen(req1)\n",
    "                content = str(resp1.read())\n",
    "                l1 = content.find('AMhZZ')\n",
    "                l2 = content.find('&', l1)\n",
    "                urll = content[l1:l2]\n",
    "\n",
    "                newurl = \"https://www.google.com/search?tbs=sbi:\" + urll + \"&site=search&sa=X\"\n",
    "                req2 = urllib2.Request(newurl, headers=headers)\n",
    "                resp2 = urllib2.urlopen(req2)\n",
    "                l3 = content.find('/search?sa=X&amp;q=')\n",
    "                l4 = content.find(';', l3 + 19)\n",
    "                urll2 = content[l3 + 19:l4]\n",
    "                return (urll2)\n",
    "            except:\n",
    "                return \"Cloud not connect to Google Images endpoint\"\n",
    "\n",
    "    # Building URL parameters\n",
    "    def build_url_parameters(self, arguments):\n",
    "        if arguments['language']:\n",
    "            lang = \"&lr=\"\n",
    "            lang_param = {\"Arabic\": \"lang_ar\", \"Chinese (Simplified)\": \"lang_zh-CN\",\n",
    "                          \"Chinese (Traditional)\": \"lang_zh-TW\", \"Czech\": \"lang_cs\", \"Danish\": \"lang_da\",\n",
    "                          \"Dutch\": \"lang_nl\", \"English\": \"lang_en\", \"Estonian\": \"lang_et\", \"Finnish\": \"lang_fi\",\n",
    "                          \"French\": \"lang_fr\", \"German\": \"lang_de\", \"Greek\": \"lang_el\", \"Hebrew\": \"lang_iw \",\n",
    "                          \"Hungarian\": \"lang_hu\", \"Icelandic\": \"lang_is\", \"Italian\": \"lang_it\", \"Japanese\": \"lang_ja\",\n",
    "                          \"Korean\": \"lang_ko\", \"Latvian\": \"lang_lv\", \"Lithuanian\": \"lang_lt\", \"Norwegian\": \"lang_no\",\n",
    "                          \"Portuguese\": \"lang_pt\", \"Polish\": \"lang_pl\", \"Romanian\": \"lang_ro\", \"Russian\": \"lang_ru\",\n",
    "                          \"Spanish\": \"lang_es\", \"Swedish\": \"lang_sv\", \"Turkish\": \"lang_tr\"}\n",
    "            lang_url = lang + lang_param[arguments['language']]\n",
    "        else:\n",
    "            lang_url = ''\n",
    "\n",
    "\n",
    "        built_url = \"&tbs=\"\n",
    "        counter = 0\n",
    "        params = {'color': [arguments['color'], {'red': 'ic:specific,isc:red', 'orange': 'ic:specific,isc:orange',\n",
    "                                                 'yellow': 'ic:specific,isc:yellow', 'green': 'ic:specific,isc:green',\n",
    "                                                 'teal': 'ic:specific,isc:teel', 'blue': 'ic:specific,isc:blue',\n",
    "                                                 'purple': 'ic:specific,isc:purple', 'pink': 'ic:specific,isc:pink',\n",
    "                                                 'white': 'ic:specific,isc:white', 'gray': 'ic:specific,isc:gray',\n",
    "                                                 'black': 'ic:specific,isc:black', 'brown': 'ic:specific,isc:brown'}],\n",
    "                  'color_type': [arguments['color_type'],\n",
    "                                 {'full-color': 'ic:color', 'black-and-white': 'ic:gray', 'transparent': 'ic:trans'}],\n",
    "                  'usage_rights': [arguments['usage_rights'],\n",
    "                                   {'labeled-for-reuse-with-modifications': 'sur:fmc', 'labeled-for-reuse': 'sur:fc',\n",
    "                                    'labeled-for-noncommercial-reuse-with-modification': 'sur:fm',\n",
    "                                    'labeled-for-nocommercial-reuse': 'sur:f'}],\n",
    "                  'size': [arguments['size'],\n",
    "                           {'large': 'isz:l', 'medium': 'isz:m', 'icon': 'isz:i', '>400*300': 'isz:lt,islt:qsvga',\n",
    "                            '>640*480': 'isz:lt,islt:vga', '>800*600': 'isz:lt,islt:svga',\n",
    "                            '>1024*768': 'visz:lt,islt:xga', '>2MP': 'isz:lt,islt:2mp', '>4MP': 'isz:lt,islt:4mp',\n",
    "                            '>6MP': 'isz:lt,islt:6mp', '>8MP': 'isz:lt,islt:8mp', '>10MP': 'isz:lt,islt:10mp',\n",
    "                            '>12MP': 'isz:lt,islt:12mp', '>15MP': 'isz:lt,islt:15mp', '>20MP': 'isz:lt,islt:20mp',\n",
    "                            '>40MP': 'isz:lt,islt:40mp', '>70MP': 'isz:lt,islt:70mp'}],\n",
    "                  'type': [arguments['type'], {'face': 'itp:face', 'photo': 'itp:photo', 'clipart': 'itp:clipart',\n",
    "                                               'line-drawing': 'itp:lineart', 'animated': 'itp:animated'}],\n",
    "                  'time': [arguments['time'], {'past-24-hours': 'qdr:d', 'past-7-days': 'qdr:w', 'past-month': 'qdr:m',\n",
    "                                               'past-year': 'qdr:y'}],\n",
    "                  'aspect_ratio': [arguments['aspect_ratio'],\n",
    "                                   {'tall': 'iar:t', 'square': 'iar:s', 'wide': 'iar:w', 'panoramic': 'iar:xw'}],\n",
    "                  'format': [arguments['format'],\n",
    "                             {'jpg': 'ift:jpg', 'gif': 'ift:gif', 'png': 'ift:png', 'bmp': 'ift:bmp', 'svg': 'ift:svg',\n",
    "                              'webp': 'webp', 'ico': 'ift:ico', 'raw': 'ift:craw'}]}\n",
    "        for key, value in params.items():\n",
    "            if value[0] is not None:\n",
    "                ext_param = value[1][value[0]]\n",
    "                # counter will tell if it is first param added or not\n",
    "                if counter == 0:\n",
    "                    # add it to the built url\n",
    "                    built_url = built_url + ext_param\n",
    "                    counter += 1\n",
    "                else:\n",
    "                    built_url = built_url + ',' + ext_param\n",
    "                    counter += 1\n",
    "        built_url = lang_url + built_url\n",
    "        return built_url\n",
    "\n",
    "    # building main search URL\n",
    "    def build_search_url(self, search_term, params, url, similar_images, specific_site, safe_search):\n",
    "        # check safe_search\n",
    "        safe_search_string = \"&safe=active\"\n",
    "        # check the args and choose the URL\n",
    "        if url:\n",
    "            url = url\n",
    "        elif similar_images:\n",
    "            print(similar_images)\n",
    "            keywordem = self.similar_images(similar_images)\n",
    "            url = 'https://www.google.com/search?q=' + keywordem + '&espv=2&biw=1366&bih=667&site=webhp&source=lnms&tbm=isch&sa=X&ei=XosDVaCXD8TasATItgE&ved=0CAcQ_AUoAg'\n",
    "        elif specific_site:\n",
    "            url = 'https://www.google.com/search?q=' + quote(\n",
    "                search_term.encode(\n",
    "                    'utf-8')) + '&as_sitesearch=' + specific_site + '&espv=2&biw=1366&bih=667&site=webhp&source=lnms&tbm=isch' + params + '&sa=X&ei=XosDVaCXD8TasATItgE&ved=0CAcQ_AUoAg'\n",
    "        else:\n",
    "            url = 'https://www.google.com/search?q=' + quote(\n",
    "                search_term.encode(\n",
    "                    'utf-8')) + '&espv=2&biw=1366&bih=667&site=webhp&source=lnms&tbm=isch' + params + '&sa=X&ei=XosDVaCXD8TasATItgE&ved=0CAcQ_AUoAg'\n",
    "\n",
    "        # safe search check\n",
    "        if safe_search:\n",
    "            url = url + safe_search_string\n",
    "\n",
    "        return url\n",
    "\n",
    "    # measures the file size\n",
    "    def file_size(self, file_path):\n",
    "        if os.path.isfile(file_path):\n",
    "            file_info = os.stat(file_path)\n",
    "            size = file_info.st_size\n",
    "            for x in ['bytes', 'KB', 'MB', 'GB', 'TB']:\n",
    "                if size < 1024.0:\n",
    "                    return \"%3.1f %s\" % (size, x)\n",
    "                size /= 1024.0\n",
    "            return size\n",
    "\n",
    "    # keywords from file\n",
    "    def keywords_from_file(self, file_name):\n",
    "        search_keyword = []\n",
    "        with codecs.open(file_name, 'r', encoding='utf-8-sig') as f:\n",
    "            if '.csv' in file_name:\n",
    "                for line in f:\n",
    "                    if line in ['\\n', '\\r\\n']:\n",
    "                        pass\n",
    "                    else:\n",
    "                        search_keyword.append(line.replace('\\n', '').replace('\\r', ''))\n",
    "            elif '.txt' in file_name:\n",
    "                for line in f:\n",
    "                    if line in ['\\n', '\\r\\n']:\n",
    "                        pass\n",
    "                    else:\n",
    "                        search_keyword.append(line.replace('\\n', '').replace('\\r', ''))\n",
    "            else:\n",
    "                print(\"Invalid file type: Valid file types are either .txt or .csv \\n\"\n",
    "                      \"exiting...\")\n",
    "                sys.exit()\n",
    "        return search_keyword\n",
    "\n",
    "    # make directories\n",
    "    def create_directories(self, main_directory, dir_name, thumbnail, thumbnail_only):\n",
    "        dir_name_thumbnail = dir_name + \" - thumbnail\"\n",
    "        # make a search keyword  directory\n",
    "        try:\n",
    "            if not os.path.exists(main_directory):\n",
    "                os.makedirs(main_directory)\n",
    "                time.sleep(0.15)\n",
    "                path = (dir_name)\n",
    "                sub_directory = os.path.join(main_directory, path)\n",
    "                if not os.path.exists(sub_directory):\n",
    "                    os.makedirs(sub_directory)\n",
    "                if thumbnail or thumbnail_only:\n",
    "                    sub_directory_thumbnail = os.path.join(main_directory, dir_name_thumbnail)\n",
    "                    if not os.path.exists(sub_directory_thumbnail):\n",
    "                        os.makedirs(sub_directory_thumbnail)\n",
    "            else:\n",
    "                path = (dir_name)\n",
    "                sub_directory = os.path.join(main_directory, path)\n",
    "                if not os.path.exists(sub_directory):\n",
    "                    os.makedirs(sub_directory)\n",
    "                if thumbnail or thumbnail_only:\n",
    "                    sub_directory_thumbnail = os.path.join(main_directory, dir_name_thumbnail)\n",
    "                    if not os.path.exists(sub_directory_thumbnail):\n",
    "                        os.makedirs(sub_directory_thumbnail)\n",
    "        except OSError as e:\n",
    "            if e.errno != 17:\n",
    "                raise\n",
    "            pass\n",
    "        return\n",
    "\n",
    "    # Download Image thumbnails\n",
    "    def download_image_thumbnail(self, image_url, main_directory, dir_name, return_image_name, print_urls,\n",
    "                                 socket_timeout, print_size, no_download, save_source, img_src, ignore_urls):\n",
    "        if print_urls or no_download:\n",
    "            print(\"Image URL: \" + image_url)\n",
    "        if no_download:\n",
    "            return \"success\", \"Printed url without downloading\"\n",
    "        try:\n",
    "            req = Request(image_url, headers={\n",
    "                \"User-Agent\": \"Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.17 (KHTML, like Gecko) Chrome/24.0.1312.27 Safari/537.17\"})\n",
    "            try:\n",
    "                # timeout time to download an image\n",
    "                if socket_timeout:\n",
    "                    timeout = float(socket_timeout)\n",
    "                else:\n",
    "                    timeout = 10\n",
    "\n",
    "                response = urlopen(req, None, timeout)\n",
    "                data = response.read()\n",
    "                response.close()\n",
    "\n",
    "                path = main_directory + \"/\" + dir_name + \" - thumbnail\" + \"/\" + return_image_name\n",
    "\n",
    "                try:\n",
    "                    output_file = open(path, 'wb')\n",
    "                    output_file.write(data)\n",
    "                    output_file.close()\n",
    "                    if save_source:\n",
    "                        list_path = main_directory + \"/\" + save_source + \".txt\"\n",
    "                        list_file = open(list_path, 'a')\n",
    "                        list_file.write(path + '\\t' + img_src + '\\n')\n",
    "                        list_file.close()\n",
    "                except OSError as e:\n",
    "                    download_status = 'fail'\n",
    "                    download_message = \"OSError on an image...trying next one...\" + \" Error: \" + str(e)\n",
    "                except IOError as e:\n",
    "                    download_status = 'fail'\n",
    "                    download_message = \"IOError on an image...trying next one...\" + \" Error: \" + str(e)\n",
    "\n",
    "                download_status = 'success'\n",
    "                download_message = \"Completed Image Thumbnail ====> \" + return_image_name\n",
    "\n",
    "                # image size parameter\n",
    "                if print_size:\n",
    "                    print(\"Image Size: \" + str(self.file_size(path)))\n",
    "\n",
    "            except UnicodeEncodeError as e:\n",
    "                download_status = 'fail'\n",
    "                download_message = \"UnicodeEncodeError on an image...trying next one...\" + \" Error: \" + str(e)\n",
    "\n",
    "        except HTTPError as e:  # If there is any HTTPError\n",
    "            download_status = 'fail'\n",
    "            download_message = \"HTTPError on an image...trying next one...\" + \" Error: \" + str(e)\n",
    "\n",
    "        except URLError as e:\n",
    "            download_status = 'fail'\n",
    "            download_message = \"URLError on an image...trying next one...\" + \" Error: \" + str(e)\n",
    "\n",
    "        except ssl.CertificateError as e:\n",
    "            download_status = 'fail'\n",
    "            download_message = \"CertificateError on an image...trying next one...\" + \" Error: \" + str(e)\n",
    "\n",
    "        except IOError as e:  # If there is any IOError\n",
    "            download_status = 'fail'\n",
    "            download_message = \"IOError on an image...trying next one...\" + \" Error: \" + str(e)\n",
    "        return download_status, download_message\n",
    "\n",
    "    # Download Images\n",
    "    def download_image(self, image_url, image_format, main_directory, dir_name, count, print_urls, socket_timeout,\n",
    "                       prefix, print_size, no_numbering, no_download, save_source, img_src, silent_mode, thumbnail_only,\n",
    "                       format, ignore_urls):\n",
    "        if not silent_mode:\n",
    "            if print_urls or no_download:\n",
    "                print(\"Image URL: \" + image_url)\n",
    "        if ignore_urls:\n",
    "            if any(url in image_url for url in ignore_urls.split(',')):\n",
    "                return \"fail\", \"Image ignored due to 'ignore url' parameter\", None, image_url\n",
    "        if thumbnail_only:\n",
    "            return \"success\", \"Skipping image download...\", str(image_url[(image_url.rfind('/')) + 1:]), image_url\n",
    "        if no_download:\n",
    "            return \"success\", \"Printed url without downloading\", None, image_url\n",
    "        try:\n",
    "            req = Request(image_url, headers={\n",
    "                \"User-Agent\": \"Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.17 (KHTML, like Gecko) Chrome/24.0.1312.27 Safari/537.17\"})\n",
    "            try:\n",
    "                # timeout time to download an image\n",
    "                if socket_timeout:\n",
    "                    timeout = float(socket_timeout)\n",
    "                else:\n",
    "                    timeout = 10\n",
    "\n",
    "                response = urlopen(req, None, timeout)\n",
    "                data = response.read()\n",
    "                info = response.info()\n",
    "                response.close()\n",
    "\n",
    "                qmark = image_url.rfind('?')\n",
    "                if qmark == -1:\n",
    "                    qmark = len(image_url)\n",
    "                slash = image_url.rfind('/', 0, qmark) + 1\n",
    "                image_name = str(image_url[slash:qmark]).lower()\n",
    "\n",
    "                type = info.get_content_type()\n",
    "                if type == \"image/jpeg\" or type == \"image/jpg\":\n",
    "                    if not image_name.endswith(\".jpg\") and not image_name.endswith(\".jpeg\"):\n",
    "                        image_name += \".jpg\"\n",
    "                elif type == \"image/png\":\n",
    "                    if not image_name.endswith(\".png\"):\n",
    "                        image_name += \".png\"\n",
    "                elif type == \"image/webp\":\n",
    "                    if not image_name.endswith(\".webp\"):\n",
    "                        image_name += \".webp\"\n",
    "                elif type == \"image/gif\":\n",
    "                    if not image_name.endswith(\".gif\"):\n",
    "                        image_name += \".gif\"\n",
    "                elif type == \"image/bmp\" or type == \"image/x-windows-bmp\":\n",
    "                    if not image_name.endswith(\".bmp\"):\n",
    "                        image_name += \".bmp\"\n",
    "                elif type == \"image/x-icon\" or type == \"image/vnd.microsoft.icon\":\n",
    "                    if not image_name.endswith(\".ico\"):\n",
    "                        image_name += \".ico\"\n",
    "                elif type == \"image/svg+xml\":\n",
    "                    if not image_name.endswith(\".svg\"):\n",
    "                        image_name += \".svg\"\n",
    "                else:\n",
    "                    download_status = 'fail'\n",
    "                    download_message = \"Invalid image format '\" + type + \"'. Skipping...\"\n",
    "                    return_image_name = ''\n",
    "                    absolute_path = ''\n",
    "                    return download_status, download_message, return_image_name, absolute_path\n",
    "\n",
    "                # prefix name in image\n",
    "                if prefix:\n",
    "                    prefix = prefix + \" \"\n",
    "                else:\n",
    "                    prefix = ''\n",
    "\n",
    "                if no_numbering:\n",
    "                    path = main_directory + \"/\" + dir_name + \"/\" + prefix + image_name\n",
    "                else:\n",
    "                    path = main_directory + \"/\" + dir_name + \"/\" + prefix + str(count) + \".\" + image_name\n",
    "\n",
    "                try:\n",
    "                    output_file = open(path, 'wb')\n",
    "                    output_file.write(data)\n",
    "                    output_file.close()\n",
    "                    if save_source:\n",
    "                        list_path = main_directory + \"/\" + save_source + \".txt\"\n",
    "                        list_file = open(list_path, 'a')\n",
    "                        list_file.write(path + '\\t' + img_src + '\\n')\n",
    "                        list_file.close()\n",
    "                    absolute_path = os.path.abspath(path)\n",
    "                except OSError as e:\n",
    "                    download_status = 'fail'\n",
    "                    download_message = \"OSError on an image...trying next one...\" + \" Error: \" + str(e)\n",
    "                    return_image_name = ''\n",
    "                    absolute_path = ''\n",
    "\n",
    "                # return image name back to calling method to use it for thumbnail downloads\n",
    "                download_status = 'success'\n",
    "                download_message = \"Completed Image ====> \" + prefix + str(count) + \".\" + image_name\n",
    "                return_image_name = prefix + str(count) + \".\" + image_name\n",
    "\n",
    "                # image size parameter\n",
    "                if not silent_mode:\n",
    "                    if print_size:\n",
    "                        print(\"Image Size: \" + str(self.file_size(path)))\n",
    "\n",
    "            except UnicodeEncodeError as e:\n",
    "                download_status = 'fail'\n",
    "                download_message = \"UnicodeEncodeError on an image...trying next one...\" + \" Error: \" + str(e)\n",
    "                return_image_name = ''\n",
    "                absolute_path = ''\n",
    "\n",
    "            except URLError as e:\n",
    "                download_status = 'fail'\n",
    "                download_message = \"URLError on an image...trying next one...\" + \" Error: \" + str(e)\n",
    "                return_image_name = ''\n",
    "                absolute_path = ''\n",
    "\n",
    "            except BadStatusLine as e:\n",
    "                download_status = 'fail'\n",
    "                download_message = \"BadStatusLine on an image...trying next one...\" + \" Error: \" + str(e)\n",
    "                return_image_name = ''\n",
    "                absolute_path = ''\n",
    "\n",
    "        except HTTPError as e:  # If there is any HTTPError\n",
    "            download_status = 'fail'\n",
    "            download_message = \"HTTPError on an image...trying next one...\" + \" Error: \" + str(e)\n",
    "            return_image_name = ''\n",
    "            absolute_path = ''\n",
    "\n",
    "        except URLError as e:\n",
    "            download_status = 'fail'\n",
    "            download_message = \"URLError on an image...trying next one...\" + \" Error: \" + str(e)\n",
    "            return_image_name = ''\n",
    "            absolute_path = ''\n",
    "\n",
    "        except ssl.CertificateError as e:\n",
    "            download_status = 'fail'\n",
    "            download_message = \"CertificateError on an image...trying next one...\" + \" Error: \" + str(e)\n",
    "            return_image_name = ''\n",
    "            absolute_path = ''\n",
    "\n",
    "        except IOError as e:  # If there is any IOError\n",
    "            download_status = 'fail'\n",
    "            download_message = \"IOError on an image...trying next one...\" + \" Error: \" + str(e)\n",
    "            return_image_name = ''\n",
    "            absolute_path = ''\n",
    "\n",
    "        except IncompleteRead as e:\n",
    "            download_status = 'fail'\n",
    "            download_message = \"IncompleteReadError on an image...trying next one...\" + \" Error: \" + str(e)\n",
    "            return_image_name = ''\n",
    "            absolute_path = ''\n",
    "\n",
    "        return download_status, download_message, return_image_name, absolute_path\n",
    "\n",
    "    def _get_all_items(self, image_objects, main_directory, dir_name, limit, arguments):\n",
    "        items = []\n",
    "        abs_path = []\n",
    "        errorCount = 0\n",
    "        i = 0\n",
    "        count = 1\n",
    "        while count < limit + 1 and i < len(image_objects):\n",
    "            if len(image_objects) == 0:\n",
    "                print(\"no_links\")\n",
    "                break\n",
    "            #code added here to attempt to implement offset correctly\n",
    "            #was \"count < int(arguments['offset'])\" in hardikvasa code, this seems\n",
    "            # to be contrary to the implementation details. \n",
    "            elif arguments['offset'] and count <= int(arguments['offset']):\n",
    "                    count += 1\n",
    "                    #page = page[end_content:]\n",
    "            else:\n",
    "                # format the item for readability\n",
    "                object = self.format_object(image_objects[i])\n",
    "                \n",
    "                if arguments['metadata']:\n",
    "                    if not arguments[\"silent_mode\"]:\n",
    "                        print(\"\\nImage Metadata: \" + str(object))\n",
    "\n",
    "                # download the images\n",
    "                download_status, download_message, return_image_name, absolute_path = self.download_image(\n",
    "                    object['image_link'], object['image_format'], main_directory, dir_name, count,\n",
    "                    arguments['print_urls'], arguments['socket_timeout'], arguments['prefix'], arguments['print_size'],\n",
    "                    arguments['no_numbering'], arguments['no_download'], arguments['save_source'],\n",
    "                    object['image_source'], arguments[\"silent_mode\"], arguments[\"thumbnail_only\"], arguments['format'],\n",
    "                    arguments['ignore_urls'])\n",
    "                if not arguments[\"silent_mode\"]:\n",
    "                    print(download_message)\n",
    "                if download_status == \"success\":\n",
    "\n",
    "                    # download image_thumbnails\n",
    "                    if arguments['thumbnail'] or arguments[\"thumbnail_only\"]:\n",
    "                        download_status, download_message_thumbnail = self.download_image_thumbnail(\n",
    "                            object['image_thumbnail_url'], main_directory, dir_name, return_image_name,\n",
    "                            arguments['print_urls'], arguments['socket_timeout'], arguments['print_size'],\n",
    "                            arguments['no_download'], arguments['save_source'], object['image_source'],\n",
    "                            arguments['ignore_urls'])\n",
    "                        if not arguments[\"silent_mode\"]:\n",
    "                            print(download_message_thumbnail)\n",
    "\n",
    "                    count += 1\n",
    "                    object['image_filename'] = return_image_name\n",
    "                    items.append(object)  # Append all the links in the list named 'Links'\n",
    "                    abs_path.append(absolute_path)\n",
    "                else:\n",
    "                    errorCount += 1\n",
    "\n",
    "                # delay param\n",
    "                if arguments['delay']:\n",
    "                    time.sleep(int(arguments['delay']))\n",
    "            i += 1\n",
    "        if count < limit:\n",
    "            print(\"\\n\\nUnfortunately all \" + str(\n",
    "                limit) + \" could not be downloaded because some images were not downloadable. \" + str(\n",
    "                count - 1) + \" is all we got for this search filter!\")\n",
    "        return items, errorCount, abs_path\n",
    "\n",
    "    # Bulk Download\n",
    "    def download(self, arguments):\n",
    "        paths_agg = {}\n",
    "        # for input coming from other python files\n",
    "        if __name__ != \"__main__\":\n",
    "            # if the calling file contains config_file param\n",
    "            if 'config_file' in arguments:\n",
    "                records = []\n",
    "                json_file = json.load(open(arguments['config_file']))\n",
    "                for record in range(0, len(json_file['Records'])):\n",
    "                    arguments = {}\n",
    "                    for i in args_list:\n",
    "                        arguments[i] = None\n",
    "                    for key, value in json_file['Records'][record].items():\n",
    "                        arguments[key] = value\n",
    "                    records.append(arguments)\n",
    "                total_errors = 0\n",
    "                for rec in records:\n",
    "                    paths, errors = self.download_executor(rec)\n",
    "                    for i in paths:\n",
    "                        paths_agg[i] = paths[i]\n",
    "                    if not arguments[\"silent_mode\"]:\n",
    "                        if arguments['print_paths']:\n",
    "                            print(paths.encode('raw_unicode_escape').decode('utf-8'))\n",
    "                    total_errors = total_errors + errors\n",
    "                return paths_agg, total_errors\n",
    "            # if the calling file contains params directly\n",
    "            else:\n",
    "                paths, errors = self.download_executor(arguments)\n",
    "                for i in paths:\n",
    "                    paths_agg[i] = paths[i]\n",
    "                if not arguments[\"silent_mode\"]:\n",
    "                    if arguments['print_paths']:\n",
    "                        print(paths.encode('raw_unicode_escape').decode('utf-8'))\n",
    "                return paths_agg, errors\n",
    "        # for input coming from CLI\n",
    "        else:\n",
    "            paths, errors = self.download_executor(arguments)\n",
    "            for i in paths:\n",
    "                paths_agg[i] = paths[i]\n",
    "            if not arguments[\"silent_mode\"]:\n",
    "                if arguments['print_paths']:\n",
    "                    print(paths.encode('raw_unicode_escape').decode('utf-8'))\n",
    "        return paths_agg, errors\n",
    "\n",
    "    def download_executor(self, arguments):\n",
    "        paths = {}\n",
    "        errorCount = None\n",
    "        for arg in args_list:\n",
    "            if arg not in arguments:\n",
    "                arguments[arg] = None\n",
    "        ######Initialization and Validation of user arguments\n",
    "        if arguments['keywords']:\n",
    "            search_keyword = [str(item) for item in arguments['keywords'].split(',')]\n",
    "\n",
    "        if arguments['keywords_from_file']:\n",
    "            search_keyword = self.keywords_from_file(arguments['keywords_from_file'])\n",
    "\n",
    "        # both time and time range should not be allowed in the same query\n",
    "        if arguments['time'] and arguments['time_range']:\n",
    "            raise ValueError(\n",
    "                'Either time or time range should be used in a query. Both cannot be used at the same time.')\n",
    "\n",
    "        # both time and time range should not be allowed in the same query\n",
    "        if arguments['size'] and arguments['exact_size']:\n",
    "            raise ValueError(\n",
    "                'Either \"size\" or \"exact_size\" should be used in a query. Both cannot be used at the same time.')\n",
    "\n",
    "        # both image directory and no image directory should not be allowed in the same query\n",
    "        if arguments['image_directory'] and arguments['no_directory']:\n",
    "            raise ValueError('You can either specify image directory or specify no image directory, not both!')\n",
    "\n",
    "        # Additional words added to keywords\n",
    "        if arguments['suffix_keywords']:\n",
    "            suffix_keywords = [\" \" + str(sk) for sk in arguments['suffix_keywords'].split(',')]\n",
    "        else:\n",
    "            suffix_keywords = ['']\n",
    "\n",
    "        # Additional words added to keywords\n",
    "        if arguments['prefix_keywords']:\n",
    "            prefix_keywords = [str(sk) + \" \" for sk in arguments['prefix_keywords'].split(',')]\n",
    "        else:\n",
    "            prefix_keywords = ['']\n",
    "\n",
    "        # Setting limit on number of images to be downloaded\n",
    "        if arguments['limit']:\n",
    "            limit = int(arguments['limit'])\n",
    "        else:\n",
    "            limit = 100\n",
    "\n",
    "        if arguments['url']:\n",
    "            current_time = str(datetime.datetime.now()).split('.')[0]\n",
    "            search_keyword = [current_time.replace(\":\", \"_\")]\n",
    "\n",
    "        if arguments['similar_images']:\n",
    "            current_time = str(datetime.datetime.now()).split('.')[0]\n",
    "            search_keyword = [current_time.replace(\":\", \"_\")]\n",
    "\n",
    "        # If single_image or url argument not present then keywords is mandatory argument\n",
    "        if arguments['single_image'] is None and arguments['url'] is None and arguments['similar_images'] is None and \\\n",
    "                arguments['keywords'] is None and arguments['keywords_from_file'] is None:\n",
    "            print('-------------------------------\\n'\n",
    "                  'Uh oh! Keywords is a required argument \\n\\n'\n",
    "                  'Please refer to the documentation on guide to writing queries \\n'\n",
    "                  'https://github.com/hardikvasa/google-images-download#examples'\n",
    "                  '\\n\\nexiting!\\n'\n",
    "                  '-------------------------------')\n",
    "            sys.exit()\n",
    "\n",
    "        # If this argument is present, set the custom output directory\n",
    "        if arguments['output_directory']:\n",
    "            main_directory = arguments['output_directory']\n",
    "        else:\n",
    "            main_directory = \"downloads\"\n",
    "\n",
    "        # Proxy settings\n",
    "        if arguments['proxy']:\n",
    "            os.environ[\"http_proxy\"] = arguments['proxy']\n",
    "            os.environ[\"https_proxy\"] = arguments['proxy']\n",
    "\n",
    "        # Add time range to keywords if asked\n",
    "        time_range = ''\n",
    "        if arguments['time_range']:\n",
    "            json_acceptable_string = arguments['time_range'].replace(\"'\", \"\\\"\")\n",
    "            d = json.loads(json_acceptable_string)\n",
    "            time_range = ' after:' + d['time_min'] + ' before:' + d['time_max']\n",
    "\n",
    "        exact_size = ''\n",
    "        if arguments['exact_size']:\n",
    "            size_array = [x.strip() for x in arguments['exact_size'].split(',')]\n",
    "            exact_size = \" imagesize:\" + str(size_array[0]) + \"x\" + str(size_array[1])\n",
    "\n",
    "            ######Initialization Complete\n",
    "        total_errors = 0\n",
    "        for pky in prefix_keywords:  # 1.for every prefix keywords\n",
    "            for sky in suffix_keywords:  # 2.for every suffix keywords\n",
    "                i = 0\n",
    "                while i < len(search_keyword):  # 3.for every main keyword\n",
    "                    iteration = \"\\n\" + \"Item no.: \" + str(i + 1) + \" -->\" + \" Item name = \" + (pky) + (\n",
    "                    search_keyword[i]) + (sky)\n",
    "                    if not arguments[\"silent_mode\"]:\n",
    "                        try:\n",
    "                            print(iteration.encode('raw_unicode_escape').decode('utf-8'))\n",
    "                        except UnicodeDecodeError:\n",
    "                            print(iteration)\n",
    "                        print(\"Evaluating...\")\n",
    "                        \n",
    "                    else:\n",
    "                        print(\"Downloading images for: \" + (pky) + (search_keyword[i]) + (sky) + \" ...\")\n",
    "                    search_term = pky + search_keyword[i] + sky\n",
    "\n",
    "                    if arguments['image_directory']:\n",
    "                        dir_name = arguments['image_directory']\n",
    "                    elif arguments['no_directory']:\n",
    "                        dir_name = ''\n",
    "                    else:\n",
    "                        dir_name = search_term + (\n",
    "                            '-' + arguments['color'] if arguments['color'] else '')  # sub-directory\n",
    "\n",
    "                    if not arguments[\"no_download\"]:\n",
    "                        self.create_directories(main_directory, dir_name, arguments['thumbnail'],\n",
    "                                                arguments['thumbnail_only'])  # create directories in OS\n",
    "\n",
    "                    params = self.build_url_parameters(arguments)  # building URL with params\n",
    "\n",
    "                    search_term += time_range + exact_size\n",
    "                    url = self.build_search_url(search_term, params, arguments['url'], arguments['similar_images'],\n",
    "                                                arguments['specific_site'],\n",
    "                                                arguments['safe_search'])  # building main search url\n",
    "\n",
    "                    if limit < 101:\n",
    "                        images, tabs = self.download_page(url)  # download page\n",
    "                    else:\n",
    "                        images, tabs = self.download_extended_page(url, arguments['chromedriver'], arguments['browser'])\n",
    "\n",
    "                    if not arguments[\"silent_mode\"]:\n",
    "                        if arguments['no_download']:\n",
    "                            print(\"Getting URLs without downloading images...\")\n",
    "                        else:\n",
    "                            print(\"Starting Download...\")\n",
    "                    items, errorCount, abs_path = self._get_all_items(images, main_directory, dir_name, limit,\n",
    "                                                                      arguments)  # get all image items and download images\n",
    "                    paths[pky + search_keyword[i] + sky] = abs_path\n",
    "\n",
    "                    # dumps into a json file\n",
    "                    if arguments['extract_metadata']:\n",
    "                        try:\n",
    "                            if not os.path.exists(\"logs\"):\n",
    "                                os.makedirs(\"logs\")\n",
    "                        except OSError as e:\n",
    "                            print(e)\n",
    "                        json_file = open(\"logs/\" + search_keyword[i] + \".json\", \"w\")\n",
    "                        json.dump(items, json_file, indent=4, sort_keys=True)\n",
    "                        json_file.close()\n",
    "\n",
    "                    # Related images\n",
    "                    if arguments['related_images']:\n",
    "                        print(\"\\nGetting list of related keywords...this may take a few moments\")\n",
    "                        for key, value in tabs.items():\n",
    "                            final_search_term = (search_term + \" - \" + key)\n",
    "                            print(\"\\nNow Downloading - \" + final_search_term)\n",
    "                            if limit < 101:\n",
    "                                images, _ = self.download_page(value)  # download page\n",
    "                            else:\n",
    "                                images, _ = self.download_extended_page(value, arguments['chromedriver'], arguments['browser'])\n",
    "                            self.create_directories(main_directory, final_search_term, arguments['thumbnail'],\n",
    "                                                    arguments['thumbnail_only'])\n",
    "                            self._get_all_items(images, main_directory, search_term + \" - \" + key, limit, arguments)\n",
    "\n",
    "                    i += 1\n",
    "                    total_errors = total_errors + errorCount\n",
    "                    if not arguments[\"silent_mode\"]:\n",
    "                        print(\"\\nErrors: \" + str(errorCount) + \"\\n\")\n",
    "        return paths, total_errors\n",
    "\n",
    "\n",
    "# ------------- Main Program -------------#\n",
    "def main():\n",
    "    records = user_input()\n",
    "    total_errors = 0\n",
    "    t0 = time.time()  # start the timer\n",
    "    for arguments in records:\n",
    "\n",
    "        if arguments['single_image']:  # Download Single Image using a URL\n",
    "            response = googleimagesdownload()\n",
    "            response.single_image(arguments['single_image'])\n",
    "        else:  # or download multiple images based on keywords/keyphrase search\n",
    "            response = googleimagesdownload()\n",
    "            paths, errors = response.download(arguments)  # wrapping response in a variable just for consistency\n",
    "            total_errors = total_errors + errors\n",
    "\n",
    "        t1 = time.time()  # stop the timer\n",
    "        total_time = t1 - t0  # Calculating the total time required to crawl, find and download all the links of 60,000 images\n",
    "        if not arguments[\"silent_mode\"]:\n",
    "            print(\"\\nEverything downloaded!\")\n",
    "            print(\"Total errors: \" + str(total_errors))\n",
    "            print(\"Total time taken: \" + str(total_time) + \" Seconds\")\n",
    "\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "#    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5434c57c-29c3-4bbb-8266-1de35b8b30dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sorted_alphanumeric( l ): \n",
    "    \"\"\" Sort the given iterable in the way that humans expect.\"\"\" \n",
    "    convert = lambda text: int(text) if text.isdigit() else text \n",
    "    alphanum_key = lambda key: [ convert(c) for c in re.split('([0-9]+)', key)] \n",
    "    return sorted(l, key = alphanum_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cb86a8-137f-45f7-ac01-45f668422937",
   "metadata": {},
   "source": [
    "### Merge all pois and add a unique uid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b9f0857-d064-4894-aa35-b67f9ae6aa6a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading pois from disk\n",
      "Total pois: 5757\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'dataset/cogent'\n",
    "\n",
    "merged_path = os.path.join(data_dir, 'all_pois.json')\n",
    "json_fnames = ['parks.json', 'pois_tourism.json', 'pois.json']\n",
    "\n",
    "pois_merged = []\n",
    "\n",
    "if os.path.exists(merged_path):\n",
    "    print('loading pois from disk')\n",
    "    with open(merged_path) as f:\n",
    "        pois_merged = json.load(f)\n",
    "else:\n",
    "    for fname in json_fnames:\n",
    "        path = os.path.join(data_dir,fname)\n",
    "        with open(path) as f:\n",
    "            pois = json.load(f)\n",
    "            pois_merged.extend(pois)\n",
    "        \n",
    "    uid_counter = 0\n",
    "    for p in pois_merged:\n",
    "        p['uid'] = uid_counter\n",
    "        uid_counter += 1\n",
    "    \n",
    "    with open(merged_path,'w') as f:\n",
    "        json.dump(pois_merged, f)\n",
    "        \n",
    "print('Total pois:',len(pois_merged))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcba747-f65e-4742-afd5-ec328772b13d",
   "metadata": {},
   "source": [
    "#### Download images (saved to downloads folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b57f19-75e1-4336-a532-dfd760868431",
   "metadata": {},
   "outputs": [],
   "source": [
    "poi_img_dir = os.path.join(data_dir, 'poi_images')\n",
    "\n",
    "tourism = False\n",
    "parks = True\n",
    "\n",
    "pois_tourism = [p for p in pois_merged if 'tourism' in p]\n",
    "\n",
    "pois_parks = [p for p in pois_merged if p.get('leisure') == 'park']\n",
    "\n",
    "pois_filtered = []\n",
    "\n",
    "if tourism:\n",
    "    for p in pois_tourism:\n",
    "        if p['tourism'] == 'artwork':\n",
    "            if 'artwork_type' in p and p['artwork_type'] == 'graffiti':\n",
    "                continue\n",
    "\n",
    "        if p['tourism'] == 'hotel':\n",
    "            continue\n",
    "        if p['tourism'] == 'guest_house':\n",
    "            continue  \n",
    "        if p['tourism'] == 'apartment':\n",
    "            continue         \n",
    "        if p['tourism'] == 'hostel':\n",
    "            continue  \n",
    "        if 'name' not in p:\n",
    "            continue\n",
    "        pois_filtered.append(p)\n",
    "elif parks:\n",
    "    for p in pois_parks:\n",
    "        if 'name' not in p:\n",
    "            continue\n",
    "        pois_filtered.append(p)\n",
    "\n",
    "print('Total number of pois:',len(pois_tourism))\n",
    "print('Pois filtered:',len(pois_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12af3ada-dbc8-49cb-acc4-b0bf303d382a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in pois_filtered:\n",
    "    \n",
    "    query = p['name']\n",
    "    if tourism:\n",
    "        if 'artist_name' in p:\n",
    "            query += ' '+p['artist_name']\n",
    "    if parks:\n",
    "        query += ' Gent'\n",
    "    \n",
    "    download_dir = 'downloads/'+query\n",
    "    \n",
    "    \n",
    "    print('Query:',query)\n",
    "    \n",
    "    if os.path.exists(download_dir) and len(os.listdir(download_dir)) > 2:\n",
    "        print('already_downloaded, skipping')\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    response = googleimagesdownload()   #class instantiation\n",
    "    \n",
    "    arguments = {\"keywords\":query,\"limit\":5,\"print_urls\":False}   #creating list of arguments\n",
    "    paths = response.download(arguments)   #passing the arguments to the function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abe6f3c-9b51-4540-9bfe-c576a440f4da",
   "metadata": {},
   "source": [
    "#### Now move images to folder poi_images, resize and save as jpg\n",
    "\n",
    "image name = UID_i.jpg , where i is the image index (0,1,2,3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c86016d-c3fd-4cb7-8374-9385daea0976",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def resize(img, max_dim=2048):\n",
    "    \n",
    "    h,w = img.shape[:2]\n",
    "    \n",
    "    if h >= w and h > max_dim:\n",
    "        new_h = max_dim\n",
    "        new_w = int(w/h * new_h)\n",
    "    elif w > h and w > max_dim:\n",
    "        new_w = max_dim\n",
    "        new_h = int(h/w*new_w)\n",
    "        \n",
    "    else:\n",
    "        return img\n",
    "    \n",
    "    return cv2.resize(img, (new_w,new_h))\n",
    "\n",
    "# add support for unicode filenames with openCV\n",
    "def imread(path):\n",
    "    \n",
    "    path = path.replace('\\\\', '/')\n",
    "    img = cv2.imdecode(np.fromfile(path, dtype=np.uint8), cv2.IMREAD_UNCHANGED)\n",
    "    \n",
    "    if img is not None:\n",
    "        # if only 2 channel\n",
    "        if len(img.shape) == 2:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "        # if image has alpha channel\n",
    "        elif img.shape[2] == 4:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGRA2BGR)\n",
    "    \n",
    "    return img\n",
    "\n",
    "def read_img(path):\n",
    "    img = cv2.cvtColor(cv2.imread(path), cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (600,600))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a67e1167-af38-44a7-834e-8d8efe9fcf9b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:35<00:00,  2.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images for pois: 600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "poi_img_dir = os.path.join(data_dir, 'poi_images')\n",
    "\n",
    "for p in tqdm(pois_filtered):\n",
    "    \n",
    "    query = p['name']\n",
    "    uid = p['uid']\n",
    "    \n",
    "    if tourism:\n",
    "        if 'artist_name' in p:\n",
    "            query += ' '+p['artist_name']\n",
    "    if parks:\n",
    "        query += ' Gent'\n",
    "        \n",
    "    download_dir = 'downloads/'+query\n",
    "    \n",
    "    if os.path.exists(download_dir):\n",
    "        img_fnames = [f for f in os.listdir(download_dir) if '.ipy' not in f]\n",
    "        img_paths_old = [os.path.join(download_dir, f) for f in img_fnames]\n",
    "        \n",
    "        for i,path_old in enumerate(img_paths_old):\n",
    "            \n",
    "            path_new = os.path.join(poi_img_dir, f'{uid}_{i}.jpg')\n",
    "            if os.path.exists(path_new):\n",
    "                continue\n",
    "                \n",
    "            img = imread(path_old)\n",
    "            if img is None:\n",
    "                print('Img is None???',path_old)\n",
    "                continue\n",
    "            \n",
    "            img = resize(img)\n",
    "            cv2.imwrite(path_new, img)\n",
    "\n",
    "print('Total images for pois:',len(os.listdir(poi_img_dir)))       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d200ea76-8d88-401a-9c95-72a3723d85a7",
   "metadata": {},
   "source": [
    "### Extract image features for the poi images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c7ff0dac-6b9d-48b6-812b-686a2ee5b7e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9567327f921047e9996a40331c779520",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92fe066554d24c4c86a79abb2f3631ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55318d3fcaa6470bb2acbf5568dcdb84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f41f1446ed214ee58eb40262f1c7e71c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/1.89G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5153bfe2bca1432aa9189ec96981725b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba12124e25da464fbb2d202178f6b450",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/712M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# setup device to use\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\n",
    "print('Running on device:',device)\n",
    "\n",
    "model, vis_processors, txt_processors = load_model_and_preprocess(name=\"blip2_feature_extractor\", model_type=\"pretrain\", is_eval=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7c990cbb-000e-4082-ab6a-52957ff8786c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75/75 [01:14<00:00,  1.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving features, avg time/batch:1.0 s\n",
      "Data saved in 0.183 s\n",
      "Finished in 75.1s\n",
      "Avg time/batch: 1.0 s\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'dataset/cogent'\n",
    "img_dir = os.path.join(data_dir, 'poi_images')\n",
    "\n",
    "batch_size = 8\n",
    "save_interval = 512\n",
    "\n",
    "all_fnames = sorted_alphanumeric([f for f in os.listdir(img_dir) if f.endswith('.jpg')])\n",
    "\n",
    "h5_path = os.path.join(data_dir, 'poi_image_features.h5')\n",
    "\n",
    "\n",
    "fnames_to_save = None\n",
    "features_to_save = None\n",
    "times = []\n",
    "t_start = time.time()\n",
    "with h5py.File(h5_path, 'a') as h5:\n",
    "    for i in tqdm(range(0,len(all_fnames), batch_size)):\n",
    "        t1 = time.time()\n",
    "        batch_torch = []\n",
    "        fnames_batch =  np.array(all_fnames[i:i+batch_size])\n",
    "        img_paths_batch = [os.path.join(img_dir,f) for f in fnames_batch]\n",
    "\n",
    "        batch_pil = (Image.open(img_path).convert(\"RGB\") for img_path in img_paths_batch)                   \n",
    "        batch_torch = torch.cat([vis_processors[\"eval\"](img).unsqueeze(0).to(device) for img in batch_pil])\n",
    "        \n",
    "        features = model.extract_features({'image':batch_torch}, mode=\"image\")\n",
    "        proj = features.image_embeds_proj.cpu().numpy().astype(np.float32)\n",
    "        fnames_batch = np.array(fnames_batch, dtype=h5py.special_dtype(vlen=str))\n",
    "\n",
    "        if fnames_to_save is None:\n",
    "\n",
    "            features_to_save = proj\n",
    "            fnames_to_save = fnames_batch\n",
    "        else:\n",
    "            features_to_save = np.append(features_to_save, proj, axis=0)\n",
    "            fnames_to_save = np.append(fnames_to_save, fnames_batch, axis=0)\n",
    "\n",
    "        times.append(time.time()-t1)\n",
    "        # write chunk to file\n",
    "        if i % save_interval == 0:\n",
    "            # hdf5 storage see: https://stackoverflow.com/a/67334192/5582470\n",
    "            if i == 0:\n",
    "                # Create the dataset at first\n",
    "                h5.create_dataset('filenames', data=fnames_to_save, compression=\"gzip\", chunks=True, maxshape=(None,))\n",
    "                h5.create_dataset('features', data=features_to_save, compression=\"gzip\", chunks=True, maxshape=(None,features_to_save.shape[1],features_to_save.shape[2]))           \n",
    "            else:\n",
    "                h5['filenames'].resize((h5['filenames'].shape[0] + fnames_to_save.shape[0]), axis=0)\n",
    "                h5['filenames'][-fnames_to_save.shape[0]:] = fnames_to_save\n",
    "\n",
    "                h5['features'].resize((h5['features'].shape[0] + features_to_save.shape[0]), axis=0)\n",
    "                h5['features'][-features_to_save.shape[0]:] = features_to_save\n",
    "                #print(f'Data saved in {time.time()-t1:.3f} s')\n",
    "            \n",
    "            # reset data to save    \n",
    "            features_to_save = None\n",
    "            fnames_to_save = None\n",
    "    \n",
    "    # save final remainder of images\n",
    "    if len(fnames_to_save):\n",
    "        print(f'Saving features, avg time/batch:{np.mean(times):.1f} s')\n",
    "        t1 = time.time()\n",
    "        h5['filenames'].resize((h5['filenames'].shape[0] + fnames_to_save.shape[0]), axis=0)\n",
    "        h5['filenames'][-fnames_to_save.shape[0]:] = fnames_to_save\n",
    "\n",
    "        h5['features'].resize((h5['features'].shape[0] + features_to_save.shape[0]), axis=0)\n",
    "        h5['features'][-features_to_save.shape[0]:] = features_to_save\n",
    "        print(f'Data saved in {time.time()-t1:.3f} s')\n",
    "        \n",
    "print(f'Finished in {time.time()-t_start:.1f}s')\n",
    "print(f'Avg time/batch: {np.mean(times):.1f} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54978884-c775-4ed0-ba72-905c9dd6a873",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
